+++
categories = ["hadoop", "hdfs", "memo"]
date = "2015-04-16T23:49:14+09:00"
description = "CDH4のクラスタからCDH5へファイルをコピーするさいのメモ"
jtitle = "バージョンの異なるクラスタ間でのdistcp"
keywords = ["hadoop", "hdfs", "distcp"]
title = "distcp_20150416"

+++
# 目的
Hadoopサーバを更新するにあたってファイルを引越しする。旧サーバはCDH4で新サーバはCDH5で構成されて、ファイルをローカルに落とすことなく直接hdfsにコピーしたい。

# はまったところ
* バージョンが違うためにhdfsプロトコルではエラーが発生
* ワイルドカードでのファイル指定ができない
* ユーザフォルダじゃない場所にコピーするのでpermissionエラーが発生

# 解決策
## バージョンが違うためにhdfsプロトコルではエラーが発生
通常は
``` hadoop distcp hdfs://FROM_HOST/filedir hdfs://TO_HOST/firedire
でコピーが行われるのだが、バージョンが異なるとエラーが発生する。

そこで、コピー先(TO_HOST)にsshして以下のコマンドを実行
``` hadoop distcp hftp://FROM_HOST/filedir hdfs://TO_HOST/firedire
クラスター同士のバージョンが違う場合はfromのプロトコルをhdfsからhftpもしくはwebhdfsに変更するとコピーができる。

## ワイルドカードでのファイル指定ができない
数百のファイルをコピーする必要があったのだが、

```
hadoop distcp hftp://FROM_HOST/hoge/*.log hdfs://TO_HOST/hoge/.
```
のようにワイルドカード(*)を利用するとそんなファイルはないと怒られる。
そこで、コピー元のサーバのhdfsからコピーするファイルのリストを作成する必要がある。

```
hadoop fs -lsr hdfs:///hoge/ | grep -e *.log | awk '{gsub("hdfs", "hftp");gsub("///", "//FROM_HOST");print $0'}   > input-files.txt
```

このinput-files.txtをTO_HOSTにもっていき以下を実行でこぴーができる。

```
hadoop fs -put input-files.txt  .
hadoop distcp -f input-files.txt hdfs://TO_HOST/hoge/.
```

## ユーザフォルダじゃない場所にコピーするのでpermissionエラーが発生
次のコマンドでpermissionを変更

```
sudo -u hdfs hadoop fs -chmod -R 777 /hoge
```

